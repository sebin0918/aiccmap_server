{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from pykospacing import Spacing\n",
    "\n",
    "\n",
    "# tqdm과 pandas 통합\n",
    "tqdm.pandas()\n",
    "\n",
    "# 한국어 모델 로드\n",
    "nlp = spacy.load(\"ko_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 파일 로드\n",
    "stopwords_file = '../../../../../data/stopwords-ko.txt'\n",
    "with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "# 불용어 처리 함수 정의\n",
    "def remove_stopwords(text, stopwords):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if token.text not in stopwords and not token.is_punct and not token.is_space]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "spacing = Spacing()\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = spacing(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def text_to_vector(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')           # 입력 텍스트를 토큰화\n",
    "    with torch.no_grad():                                   # 모델에 입력하여 출력 벡터 얻기\n",
    "        outputs = model(**inputs)\n",
    "    cls_vector = outputs.last_hidden_state[0][0].numpy()    # [CLS] 토큰에 대한 벡터 추출\n",
    "    return cls_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../../../../../data/filtered_news.xlsx')\n",
    "\n",
    "# 데이터 전처리 및 토큰화\n",
    "# df = df[:100]\n",
    "df['cleaned'] = df['feature'].apply(lambda x: remove_stopwords(preprocessing(x), stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT 모델과 토크나이저 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertModel.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 텍스트를 벡터로 변환하여 데이터프레임에 추가\n",
    "df['vector'] = df['cleaned'].apply(text_to_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
