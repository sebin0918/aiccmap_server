{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6db02b-a8b2-4b1b-8a2e-9622aad9823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18db3341-66e8-4dc5-9ecf-c3b566d6f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_excel('../../data/filtered_news.xlsx')\n",
    "# df = df[:10]\n",
    "df = df.dropna(subset=['description'])\n",
    "sentences = df['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39b3a59-8566-491b-b14b-e6d408cd2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "# 토큰화 함수\n",
    "def tokenize(sentences):\n",
    "    return [' '.join(okt.morphs(sentence)) for sentence in sentences]\n",
    "\n",
    "tokenized_sentences = tokenize(sentences)\n",
    "\n",
    "# 토큰화된 문장을 정수 인코딩\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60ecdf2b-d224-4830-86f7-ff544ac39761",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(seq) for seq in sequences)                                # 패딩\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "y = np.roll(X, -1, axis=1)                                                  # 출력 레이블 생성 (여기서는 다음 단어 예측을 위해 문장을 시프트)\n",
    "\n",
    "y = y[:, -1]                                                                # 다음 단어만 타겟으로 설정\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)    # 학습 데이터와 테스트 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f077d9a-cc50-41bd-aead-2a52c764c5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56100dc6-bcfd-4333-b1c5-1905558e4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "def create_model(lstm_units=128, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_len))\n",
    "    model.add(LSTM(lstm_units, return_sequences=False))  # return_sequences=False로 설정하여 마지막 출력만 사용\n",
    "    model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32ce5769-3c0c-4b81-b6c0-d65e9339258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 5.5676\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7500 - loss: 5.5498\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 5.5299\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 5.5054\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8750 - loss: 5.4730\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8750 - loss: 5.4273\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8750 - loss: 5.3576\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8750 - loss: 5.2386\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8750 - loss: 5.0004\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8750 - loss: 4.4396\n",
      "Basic model accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "basic_model = create_model()\n",
    "basic_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "loss, accuracy = basic_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Basic model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efa02b92-e4f1-4a3b-9911-7e8f6f2270ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 그리드 설정\n",
    "param_grid = {\n",
    "    'lstm_units': [16, 32, 64, 128, 256, 512],\n",
    "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "    'batch_size': [4, 8, 16, 32, 64, 128],\n",
    "    'epochs': [10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "# 파라미터 그리드 조합 생성\n",
    "param_combinations = list(product(param_grid['lstm_units'], param_grid['optimizer'], param_grid['batch_size'], param_grid['epochs']))\n",
    "\n",
    "# 그리드 서치 구현\n",
    "best_score = -np.inf\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "936dc895-a16c-479d-80df-4f6b5ddb1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lstm_units, optimizer, batch_size, epochs in tqdm(param_combinations):\n",
    "    model = create_model(lstm_units=lstm_units, optimizer=optimizer)                # 모델 생성\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)    # 모델 학습\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)                      # 모델 평가\n",
    "    \n",
    "    if accuracy > best_score:                                                       # 최적의 파라미터 및 스코어 업데이트\n",
    "        best_score = accuracy\n",
    "        best_params = (lstm_units, optimizer, batch_size, epochs)\n",
    "\n",
    "print(f\"Best score: {best_score} with params: lstm_units={best_params[0]}, optimizer={best_params[1]}, batch_size={best_params[2]}, epochs={best_params[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3893abb2-e168-4277-83f4-dcf81f03369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 5.5664\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4167 - loss: 5.5569\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.5452 \n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.5320\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.5171 \n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.4996\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.4790 \n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.4549\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.4243\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.3886 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a60953af50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적의 파라미터로 최종 모델 학습\n",
    "best_lstm_units, best_optimizer, best_batch_size, best_epochs = best_params\n",
    "final_model = create_model(lstm_units=best_lstm_units, optimizer=best_optimizer)\n",
    "final_model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b7b27f1-5f6c-4979-9519-fc6b8eb3e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(seed_text, next_words, model, max_len, tokenizer, temperature=1.0):\n",
    "    for _ in range(next_words):\n",
    "        tokenized_seed_text = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        tokenized_seed_text = np.array(tokenized_seed_text[-max_len:]).reshape(1, -1)\n",
    "        \n",
    "        predicted = model.predict(tokenized_seed_text, verbose=0).flatten()\n",
    "        predicted = np.log(predicted + 1e-8) / temperature\n",
    "        exp_preds = np.exp(predicted)\n",
    "        probabilities = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        predicted_word_index = np.random.choice(len(probabilities), p=probabilities)\n",
    "        \n",
    "        if predicted_word_index == 0:\n",
    "            print(\"Predicted index is 0, stopping the generation.\")\n",
    "            break\n",
    "        \n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, None)\n",
    "        \n",
    "        if not predicted_word:\n",
    "            print(f\"No word found for index {predicted_word_index}, stopping the generation.\")\n",
    "            break\n",
    "        \n",
    "        seed_text += ' ' + predicted_word\n",
    "    \n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e559178-a4d4-4ecf-8e25-02ec7167192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "증시 다시 ’ 건 이번 의 주요 채 했다 우량 기존 혼합형 국 각각 cnbc 보도국 실업 장 더 꼽히는 했다 덩달아 꼽히는 덩달아 사 신규 다른 선행 원 꼽히는 실적 신규 “ 받고 30 베트남 하락 원화 해소 촉발 엔 뉴욕증시 출시 최근 신한은행 혼합형 상승 상승 커지며 것 재\n"
     ]
    }
   ],
   "source": [
    "# 테스트 예제\n",
    "seed_text = \"증시\"\n",
    "generated_text = generate_text(seed_text, 50, final_model, max_len, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc26307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 'final_lstm_model.h5'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def save_model_to_h5(model, file_name):\n",
    "    \"\"\"모델을 HDF5 형식으로 저장하는 함수\"\"\"\n",
    "    model.save(file_name)\n",
    "    print(f\"모델이 '{file_name}'로 저장되었습니다.\")\n",
    "\n",
    "# 모델 저장 예시\n",
    "file_name = 'final_lstm_model.h5'\n",
    "save_model_to_h5(final_model, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
