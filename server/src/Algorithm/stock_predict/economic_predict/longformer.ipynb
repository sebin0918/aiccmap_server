{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization as quantization\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import LongformerModel, LongformerTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "stock_df = pd.read_excel('../../data/tb_stock.xlsx')\n",
    "main_economic_df = pd.read_excel('../../data/tb_main_economic_index.xlsx')\n",
    "korea_economic_df = pd.read_excel('../../data/tb_korea_economic_indicator.xlsx')\n",
    "\n",
    "# 샘플 데이터 사용\n",
    "stock_df = stock_df[:100]\n",
    "main_economic_df = main_economic_df[:100]\n",
    "korea_economic_df = korea_economic_df[:100]\n",
    "\n",
    "# 필요한 열만 선택\n",
    "stock_df = stock_df[['sc_date', 'sc_ss_stock']]\n",
    "main_economic_df = main_economic_df[['mei_date', 'mei_gold', 'mei_sp500', 'mei_kospi']]\n",
    "korea_economic_df = korea_economic_df[['kei_date', 'kei_m2_avg', 'kei_fr']]\n",
    "\n",
    "# 열 이름 변경\n",
    "stock_df.rename(columns={'sc_date': 'date'}, inplace=True)\n",
    "main_economic_df.rename(columns={'mei_date': 'date'}, inplace=True)\n",
    "korea_economic_df.rename(columns={'kei_date': 'date'}, inplace=True)\n",
    "\n",
    "# 데이터프레임 병합\n",
    "merged_df = pd.merge(stock_df, main_economic_df, on='date', how='inner')\n",
    "merged_df = pd.merge(merged_df, korea_economic_df, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date                                               text  target\n",
      "0 2014-09-17  On 2014-09-17, gold price was 1234.40002441406...   24520\n",
      "1 2014-09-18  On 2014-09-18, gold price was 1225.69995117187...   24200\n",
      "2 2014-09-19  On 2014-09-19, gold price was 1215.30004882812...   24200\n",
      "3 2014-09-20  On 2014-09-20, gold price was 1215.30004882812...   24200\n",
      "4 2014-09-21  On 2014-09-21, gold price was 1215.30004882812...   24200\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터 생성\n",
    "merged_df['text'] = merged_df.apply(lambda row: f\"On {row['date']}, gold price was {row['mei_gold']}, S&P 500 index was {row['mei_sp500']}, KOSPI index was {row['mei_kospi']}, M2 average was {row['kei_m2_avg']}, and FR was {row['kei_fr']}.\", axis=1)\n",
    "\n",
    "# 모델의 타겟 설정\n",
    "merged_df['target'] = merged_df['sc_ss_stock']\n",
    "\n",
    "# 날짜 형식 확인 및 변환\n",
    "if not pd.api.types.is_datetime64_any_dtype(merged_df['date']):\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "    \n",
    "print(merged_df[['date', 'text', 'target']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터셋 정의\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        target = self.data.iloc[idx]['target']\n",
    "        \n",
    "        inputs = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'target': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pruning을 적용하는 모델 정의\n",
    "class PrunedStockPricePredictor(nn.Module):\n",
    "    def __init__(self, longformer_model_name):\n",
    "        super(PrunedStockPricePredictor, self).__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained(longformer_model_name)\n",
    "        self.fc         = nn.Linear(self.longformer.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs    = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs[0][:, 0, :]\n",
    "        return self.fc(cls_output)\n",
    "    \n",
    "    def apply_pruning(self, pruning_amount=0.4):\n",
    "        # Fully connected layer에 L1 가지치기 적용\n",
    "        prune.l1_unstructured(self.fc, name=\"weight\", amount=pruning_amount)\n",
    "        prune.remove(self.fc, 'weight')  # 가지치기 적용 후 pruned 상태에서 재학습을 위해 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. 데이터 준비\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "max_length = 512\n",
    "\n",
    "train_df, test_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = StockDataset(train_df, tokenizer, max_length)\n",
    "test_dataset  = StockDataset(test_df, tokenizer, max_length)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 4 기본 학습 및 가지치기 적용\n",
    "model = PrunedStockPricePredictor('allenai/longformer-base-4096')\n",
    "model.apply_pruning(pruning_amount=0.4)  # 가지치기 적용\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in tqdm(range(5)):\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids      = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target        = batch['target']\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss    = criterion(outputs.squeeze(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 완료 후 모델 저장\n",
    "torch.save(model.state_dict(), 'initial_qat_pruned_longformer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 하이퍼파라미터 튜닝을 위한 설정\n",
    "learning_rates = [1e-5, 3e-5, 5e-5]\n",
    "model_names   = ['allenai/longformer-base-4096', 'allenai/longformer-large-4096']\n",
    "\n",
    "best_model = None\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# 훈련 및 검증 데이터 분할\n",
    "def prepare_data(loader):\n",
    "    input_ids_list      = []\n",
    "    attention_mask_list = []\n",
    "    target_list         = []\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids_list.append(batch['input_ids'].numpy())\n",
    "        attention_mask_list.append(batch['attention_mask'].numpy())\n",
    "        target_list.append(batch['target'].numpy())\n",
    "    \n",
    "    input_ids = np.concatenate(input_ids_list, axis=0)\n",
    "    attention_mask = np.concatenate(attention_mask_list, axis=0)\n",
    "    targets = np.concatenate(target_list, axis=0)\n",
    "\n",
    "    return input_ids, attention_mask, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_mask, train_targets = prepare_data(train_loader)\n",
    "train_data  =  np.hstack((train_input_ids, train_attention_mask))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_targets, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 파일을 저장할 디렉토리와 파일 이름 설정\n",
    "save_directory = \"./saved_models\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "best_model_path = os.path.join(save_directory, \"best_model.pt\")\n",
    "# 초기 설정\n",
    "best_score = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 최적의 하이퍼 파라미터 찾기\n",
    "for lr in tqdm(learning_rates, desc='최고의 학습률'):\n",
    "    for model_name in tqdm(model_names, desc='최고의 모델'):\n",
    "        print(f\"Training with lr={lr}, model_name={model_name}\")\n",
    "\n",
    "        # 모델 초기화\n",
    "        model = PrunedStockPricePredictor(model_name)\n",
    "        model.apply_pruning(pruning_amount=0.4)  # 가지치기 적용\n",
    "        model.train()\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(3):\n",
    "            model.train()\n",
    "            for i in range(0, len(X_train), 4):\n",
    "                input_ids = torch.tensor(X_train[i:i+4, :512]).to(model.longformer.device)\n",
    "                attention_mask = torch.tensor(X_train[i:i+4, 512:]).to(model.longformer.device)\n",
    "                targets = torch.tensor(y_train[i:i+4]).to(model.longformer.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # 검증 단계\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X_val), 4):\n",
    "                input_ids = torch.tensor(X_val[i:i+4, :512]).to(model.longformer.device)\n",
    "                attention_mask = torch.tensor(X_val[i:i+4, 512:]).to(model.longformer.device)\n",
    "                targets = torch.tensor(y_val[i:i+4]).to(model.longformer.device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(X_val) / 4  # 평균 손실 계산\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss}\")\n",
    "        \n",
    "        # 최적의 모델 저장\n",
    "        if val_loss < best_score:\n",
    "            best_score = val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "print(f\"Best Validation Loss: {best_score}\")\n",
    "print(f\"Best model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
