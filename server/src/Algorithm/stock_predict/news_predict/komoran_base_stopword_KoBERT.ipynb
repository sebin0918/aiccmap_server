{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1cfb82-85f8-4fb7-a667-bd731f425213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기 및 함수화\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from konlpy.tag import Komoran\n",
    "from pykospacing import Spacing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertTokenizer, BertModel, ProgressCallback, Trainer, BertForSequenceClassification, TrainingArguments\n",
    "\n",
    "komoran = Komoran()\n",
    "spacing = Spacing()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# tqdm과 pandas 통합\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91096f02-9fdf-41da-9dd0-8830f1c56863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'..\\..\\data\\filtered_samsung_news_with_outcome.xlsx')\n",
    "# df = df[5801:5900]\n",
    "# 기본 불용어 불러오기\n",
    "korean_stopwords_path = '../../data/stopwords-ko.txt'\n",
    "with open(korean_stopwords_path, encoding='utf8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip() for x in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33640df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스별로 데이터 분리\n",
    "df_positive = df[df['Outcome'] == '악재']\n",
    "df_negative = df[df['Outcome'] == '호재']\n",
    "\n",
    "# 최소 클래스의 샘플 수 확인\n",
    "min_class_count = min(len(df_positive), len(df_negative))\n",
    "\n",
    "# 다운샘플링 적용\n",
    "df_positive_downsampled = resample(df_positive, \n",
    "                                   replace=False,                               # 샘플을 복원하지 않고\n",
    "                                   n_samples=min_class_count,                   # 최소 클래스의 개수로 맞추기\n",
    "                                   random_state=42)                             # 재현성을 위해 random_state 사용\n",
    "\n",
    "df_negative_downsampled = resample(df_negative, \n",
    "                                   replace=False, \n",
    "                                   n_samples=min_class_count, \n",
    "                                   random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_positive_downsampled, df_negative_downsampled])     # 다운샘플링된 데이터 결합\n",
    "df = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)         # 데이터 셔플\n",
    "print(df_balanced['Outcome'].value_counts())                                    # 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e68a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리 함수\n",
    "def preprocessing(text):\n",
    "    text = spacing(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# komoran토큰화 &불용어 처리 함수\n",
    "def remove_stopwords(text, stopwords):\n",
    "    tokens = []\n",
    "    morphs = komoran.morphs(text)\n",
    "    for token in morphs:\n",
    "        if token not in stopwords:\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "# 텍스트 전처리 및 토큰화, 불용어 처리\n",
    "cleaned_data = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    feature_text = df.loc[i, 'summary_content']\n",
    "    processed_text = preprocessing(feature_text)\n",
    "    cleaned_text = remove_stopwords(processed_text, stopwords)\n",
    "    cleaned_data.append(cleaned_text)\n",
    "df['cleaned'] = cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_labels는 분류할 클래스의 수를 지정합니다.\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b88841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스를 정의하여 데이터를 모델에 맞게 전처리합니다.\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts                                          # 전처리된 텍스트 데이터 리스트\n",
    "        self.labels = labels                                        # 라벨 데이터 리스트\n",
    "        self.tokenizer = tokenizer                                  # KoBERT 토큰나이저\n",
    "        self.max_len = max_len                                      # 최대 토큰 길이 (128로 설정)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)                                      # 데이터셋의 크기를 반환\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]                                      # 주어진 인덱스에 해당하는 텍스트 가져오기\n",
    "        label = self.labels[idx]                                    # 주어진 인덱스에 해당하는 라벨 가져오기\n",
    "\n",
    "        # 텍스트를 KoBERT 토크나이저로 인토딩하여 입력 데이터로 변환\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,                                # 특별 토큰([CLS], [SEP]) 추가\n",
    "            max_length=self.max_len,                                # 최대 토큰 길이만큼 패딩 또는 자르기\n",
    "            return_token_type_ids=False,                            # token_type_ids 반환하지 않음\n",
    "            padding='max_length',                                   # max_length만큼 패딩 적용\n",
    "            truncation=True,                                        # max_length를 초과하는 부분을 잘라냄\n",
    "            return_attention_mask=True,                             # 어텐션 마스크 반환 (패딩된 부분은 0, 나머지는 1)\n",
    "            return_tensors='pt',                                    # PyTorch 텐서로 변환\n",
    "        )       \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),           # 인코딩된 입력 ID 텐서\n",
    "            'attention_mask': encoding['attention_mask'].flatten(), # 어텐션 마스크 텐서\n",
    "            'labels': torch.tensor(label, dtype=torch.long)         # 라벨 텐서 (정수형)\n",
    "        }\n",
    "    \n",
    "# 평가지표를 계산하는 함수 정의\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # logits이 numpy.ndarry인 경우 PyTorch 텐서로 변환\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.tensor(logits)\n",
    "      \n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    acc = accuracy_score(labels, predictions.numpy())\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions.numpy(), average='weighted')\n",
    "    return {\n",
    "        'accuracy'  : acc,\n",
    "        'f1'        : f1,\n",
    "        'precision' : precision,\n",
    "        'recall'    : recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876badf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 텍스트와 라벨로 학습 및 평가 데이터셋 인스턴스 생성\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_texts   = train_df['cleaned'].tolist()\n",
    "train_labels  = label_encoder.fit_transform(train_df['Outcome'])\n",
    "\n",
    "eval_texts    = eval_df['cleaned'].tolist()\n",
    "eval_labels   = label_encoder.transform(eval_df['Outcome'])\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "eval_dataset  = TextDataset(eval_texts, eval_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fac55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 짧은 경로 설정 (예: C:/logs)\n",
    "logging_dir = os.path.abspath('C:/logs')\n",
    "\n",
    "if os.path.exists(logging_dir):\n",
    "    shutil.rmtree(logging_dir)\n",
    "\n",
    "os.makedirs(logging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                 = './result',                         # 학습 결과가 저장될 디렉토리\n",
    "    num_train_epochs           =3,                                   # 학습을 반복할 에폭 수\n",
    "    per_device_train_batch_size=16,                                  # 학습 시 배치 크기\n",
    "    per_device_eval_batch_size =16,                                  # 평가 시 배치 크기\n",
    "    warmup_steps               =500,                                 # 학습 초기 단계에서 학습률을 서서히 증가하는 단계 수\n",
    "    weight_decay               =0.01,                                # 가중치 감쇠 (L2 정규화) 비율\n",
    "    logging_dir                =logging_dir,                         # 학습 로그가 저장될 디렉토리\n",
    "    logging_steps              =10,                                  # 몇 스텝마다 로그를 남길지 설정\n",
    "    evaluation_strategy        ='steps',                             # 평가 전략 (학습 중 주기적으로 평가)       \n",
    "    save_total_limit           =2                                    # 저장할 체크포인트 파일의 개수를 제한\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14889c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 클래스 설정\n",
    "# Trainer는 학습을 쉽게 관리할 수 있게 해주는 Hugging Face의 유틸리티 클래스\n",
    "trainer = Trainer(\n",
    "    model          = model,\n",
    "    args           = training_args,\n",
    "    train_dataset  = train_dataset,\n",
    "eval_dataset   = eval_dataset,\n",
    "    compute_metrics= compute_metrics,\n",
    "    # callbacks    = [ProgressCallback]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 그리드 정의\n",
    "learning_rates = [2e-5, 3e-5]\n",
    "batch_sizes    = [16, 32, 64]\n",
    "epochs         = [2, 4]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params   = None\n",
    "param_grid = list(itertools.product(learning_rates, batch_sizes, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6bfe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 파라미터 조합에 대해 학습 및 평가\n",
    "for lr, batch_size, epoch in tqdm(param_grid, desc=\"Find Best Param\"):\n",
    "    print(f\"Testing: Learning Rate={lr}, Batch_Size={batch_size}, Epochs={epoch}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir                 ='./results',\n",
    "        num_train_epochs           = epoch,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size =batch_size,\n",
    "        warmup_steps               =500,\n",
    "        weight_decay               =0.01,\n",
    "        logging_dir                =logging_dir,\n",
    "        logging_steps              =10,\n",
    "        evaluation_strategy        ='epoch',\n",
    "        learning_rate              =lr\n",
    "    )\n",
    "\n",
    "    # Trainer 설정\n",
    "    trainer = Trainer(\n",
    "        model                      =model,\n",
    "        args                       =training_args,\n",
    "        train_dataset              =train_dataset,\n",
    "        eval_dataset               =eval_dataset,\n",
    "        compute_metrics            =compute_metrics\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    trainer.train()\n",
    "\n",
    "    # 모델 평가\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # 최적의 하이퍼파라미터 조합을 업데이트\n",
    "    if eval_results['eval_accuracy'] > best_accuracy:\n",
    "        best_accuracy = eval_results['eval_accuracy']\n",
    "        best_params = (lr, batch_size, epochs)\n",
    "\n",
    "    print(f\"Accuracy: {eval_results['eval_accuracy']}\")\n",
    "\n",
    "print(f\"Best Params: Learning Rate={best_params[0]}, Batch_size={best_params[1]}, Epochs={best_params[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b43d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 하이퍼파라미터 모델 학습\n",
    "best_lr, best_batch_size, best_epoc = best_params\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                 ='./results',\n",
    "    num_train_epochs           = epoch,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size =batch_size,\n",
    "    warmup_steps               =500,\n",
    "    weight_decay               =0.01,\n",
    "    logging_dir                =logging_dir,\n",
    "    logging_steps              =10,\n",
    "    evaluation_strategy        ='epoch',\n",
    "    learning_rate              =lr\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 모델 평가 및 예측\n",
    "predictions, labels, _ = trainer.predict(eval_dataset)\n",
    "predictions = torch.tensor(predictions)\n",
    "predictions = torch.argmax(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동행렬 계산\n",
    "conf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "# 혼동행렬 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82232dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인덱스와 실제 라벨명을 매핑한 딕셔너리\n",
    "label_map = {0: '악재', 1: '호재'}\n",
    "\n",
    "# 텍스트를 입력 받아 예측하는 함수 정의\n",
    "def predict_text(text, model, tokenizer, max_len=128):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens   =True,\n",
    "        max_length           =max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding              ='max_length',\n",
    "        truncation           =True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors       ='pt',\n",
    "    )\n",
    "    input_ids      = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs    = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits     = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트할 텍스트 입력\n",
    "text_to_predict = '''[서울와이어 천성윤 기자] 삼성전자 최대 노조인 전국삼성전자노동조합(삼성노조)이 29일 파업을 선언했다. 삼성전자에서 파업은 창사 이래 처음이다.  \n",
    "\n",
    "삼성노조는 이날 서울 서초구 삼성전자 서초사옥 앞에서 기자회견을 열고 “노동자들을 무시하는 사측의 태도에 파업을 선언한다”고 밝혔다.\n",
    "\n",
    "삼성노조의 파업 선언은 전날 올해 임금협상을 위한 교섭에서 사측과 이견이 좁혀지지 않으며 파행한 지 하루만에 이뤄졌다. 전날 교섭에서 노사 양측은 사측 위원 2명의 교섭 참여를 놓고 갈등을 빚었다. 이 문제 때문에 정작 핵심인 임금협상 관련 중요 내용은 오가지도 못한 것으로 알려졌다.\n",
    "\n",
    "삼성노조는 “사측이 교섭에 아무런 안건도 준비하지 않고 나왔다”며 파업 선언에 이르기까지의 책임을 사측에 돌렸다.\n",
    "\n",
    "현재 삼성노조 조합원 수는 2만8000여명으로 삼성전자 전체 직원(약 12만5000명)의 22% 수준이다. 이들이 파업에 돌입함으로서 실적 개선을 이어가야 하는 삼성전자는 큰 타격을 입을 것으로 예상된다. \n",
    "\n",
    "삼성전자는 지난해 반도체 업황 부진으로 디바이스솔루션(DS) 부문에서 14조8800억원의 적자를 기록했다. 올해 1분기는 매출 71조9200억원, 영업이익 6조6100억원으로 상승세에 올라탔다.\n",
    "\n",
    "삼성노조는 즉각적인 총파업에 나서는 대신 연차 소진 등의 방식으로 단체행동을 이어갈 예정이다. 삼성노조 집행부는 조합원들에게 오는 6월 7일 하루 연차를 소진하라는 지침을 전달했다.\n",
    "\n",
    "또 이날부터 서초사옥 앞에서 버스 숙박 농성을 진행한다. 삼성노조 측은 “아직은 소극적인 파업으로 볼 수 있지만, 단계를 밟아나가겠다”면서 “총파업까지 갈 수 있고, 파업이 실패할 수도 있지만 1호 파업 행동 자체가 의미 있다”고 밝혔다.\n",
    "\n",
    "출처 : 서울와이어(http://www.seoulwire.com)'''\n",
    "\n",
    "# 예측\n",
    "predicted_label_index = predict_text(text_to_predict, model, tokenizer)\n",
    "\n",
    "# 예측 결과 출력\n",
    "predicted_label = label_map[predicted_label_index]\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 가중치 저장\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "print(\"Model has been saved as model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
